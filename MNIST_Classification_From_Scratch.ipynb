{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal # for calculating convolutions \n",
    "\n",
    "from skimage.measure import block_reduce # for calculating maxpooling\n",
    "import time \n",
    "import scipy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)  Neural network implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(output_size, input_size)\n",
    "        self.bias = np.random.randn(output_size, 1)\n",
    "        \n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input_data = input_data\n",
    "        return (np.dot(self.weights, self.input_data) + self.bias)\n",
    "    \n",
    "    def backward_propagation(self, output_gradient, learning_rate):\n",
    "        weight_gradient = np.dot(output_gradient, self.input_data.T)\n",
    "        self.weights -= learning_rate * weight_gradient\n",
    "        self.bias -= learning_rate * output_gradient\n",
    "        return np.dot(self.weights.T, output_gradient)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution():\n",
    "    def __init__(self, input_dimensions, filter_size, output_depth):\n",
    "        self.output_depth = output_depth # output depth = number of filters \n",
    "        self.input_depth, self.input_height, self.input_width = input_dimensions\n",
    "        self.input_dimensions = input_dimensions\n",
    "        self.output_dimensions = (output_depth, self.input_height - filter_size + 1, self.input_width - filter_size + 1)\n",
    "        self.filter_dimensions = (output_depth, self.input_depth, filter_size, filter_size)\n",
    "        self.filters = np.random.randn(*self.filter_dimensions)\n",
    "        self.biases = np.random.randn(*self.output_dimensions)\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input_data = input_data\n",
    "        self.output = np.copy(self.biases)\n",
    "        for i in range(self.output_depth):\n",
    "            for j in range(self.input_depth):\n",
    "                self.output[i] += signal.correlate2d(self.input_data[j], self.filters[i, j], \"valid\")\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_gradient, learning_rate):\n",
    "        grad_filters = np.zeros(self.filter_dimensions)\n",
    "        grad_input = np.zeros(self.input_dimensions)\n",
    "        for i in range(self.output_depth):\n",
    "            for j in range(self.input_depth):\n",
    "                grad_filters[i, j] = signal.correlate2d(self.input_data[j], output_gradient[i], \"valid\")\n",
    "                grad_input[j] += signal.correlate2d(output_gradient[i], self.filters[i, j], \"full\")\n",
    "        grad_biases = output_gradient\n",
    "        # Update\n",
    "        self.filters -= learning_rate * grad_filters\n",
    "        self.biases -= learning_rate * grad_biases\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization:\n",
    " * `__init__` method : initializes the convolutional layer.\n",
    "* `input_dimensions` : Represents the dimensions of the input data (depth, height, width).\n",
    "* `filter_size` : Specifies the size of the filters used in the convolution.\n",
    "* `output_depth`: Indicates the number of filters to be applied.\n",
    "\n",
    "It sets up the dimensions for the filters, biases, and initializes them randomly.\n",
    "\n",
    "#### Forward Propagation:\n",
    "* `forward_propagation` method computes the output of the convolutional layer given an input.\n",
    "* `input_data` : Represents the input data passed to the layer.\n",
    "\n",
    "It performs a correlation operation between the input data and filters to generate the output feature maps.\n",
    "the forward propagation loops through each filter and correlates it with corresponding input channels to produce the output.\n",
    "\n",
    "#### Backward Propagation:\n",
    "* `backward_propagation` method calculates gradients and updates parameters during backpropagation.\n",
    "* `output_gradien`: Represents the gradient coming from the subsequent layers.\n",
    "* `learning_rate`: Specifies the rate at which the model learns.\n",
    "\n",
    "Calculates gradients of filters and input data with respect to the loss.\n",
    "The Backward Propagation loops through each filter and computes its gradient by correlating the input and output gradients.It updates the filters and biases based on these gradients and the learning rate and returns the gradient with respect to the input for further backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maxpooling():\n",
    "    def __init__(self, input_dimensions, filter_size, stride): \n",
    "        # Initializing the Maxpooling layer with input dimensions, filter size, and stride\n",
    "        self.input_depth, self.input_height, self.input_width = input_dimensions\n",
    "        self.input_dimensions = input_dimensions \n",
    "        self.filter_size = filter_size \n",
    "        self.filter_h, self.filter_w = filter_size \n",
    "        self.stride = stride\n",
    "        \n",
    "        # Calculating output dimensions based on input dimensions, filter size, and stride\n",
    "        self.output_height = int(1 + (self.input_height - self.filter_h) / stride)\n",
    "        self.output_width = int(1 + (self.input_width - self.filter_w) / stride)\n",
    "        self.output_depth = self.input_depth \n",
    "        \n",
    "    def forward_propagation(self, input_data):\n",
    "        # Performing forward propagation, computing the output of the Maxpooling layer\n",
    "        self.input_data = input_data\n",
    "        output = np.zeros((self.output_depth, self.output_height, self.output_width))\n",
    "        stride = self.stride\n",
    "        \n",
    "        # Looping through each depth, height, and width of the output\n",
    "        for c in range(self.output_depth): \n",
    "            for i in range(self.output_height):\n",
    "                for j in range(self.output_width):\n",
    "                    # Applying max pooling operation to get the maximum value within the filter window\n",
    "                    output[c, i, j] = np.max(input_data[c, i * stride: i * stride + self.filter_h, j * stride: j * stride + self.filter_w]) \n",
    "        return output\n",
    "\n",
    "    def backward_propagation(self, output_gradient, learning_rate):\n",
    "        # Performing backward propagation to calculate gradients during backpropagation\n",
    "        grad_input = np.zeros((self.input_depth, self.input_height, self.input_width))\n",
    "        input_data = self.input_data\n",
    "        stride = self.stride\n",
    "        \n",
    "        # Looping through each depth, height, and width of the output to compute gradients\n",
    "        for c in range(self.output_depth):\n",
    "            for i in range(self.output_height):\n",
    "                for j in range(self.output_width):\n",
    "                    # Finding the indices of the max value within the filter window\n",
    "                    interm = input_data[c, i * stride: i * stride + self.filter_h, j * stride: j * stride + self.filter_w]\n",
    "                    i_max, j_max = np.where(np.max(interm) == interm)\n",
    "                    i_max, j_max = i_max[0], j_max[0]\n",
    "                    # Assigning the output gradients to the corresponding indices in the input gradients\n",
    "                    grad_input[c, i * stride: i * stride + self.filter_h, j * stride: j * stride + self.filter_w][i_max, j_max] = output_gradient[c, i, j]\n",
    "\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout():\n",
    "    def __init__(self, q_bernoulli):\n",
    "        # Initializing the Dropout layer with a probability parameter\n",
    "        self.p_bernoulli = 1 - q_bernoulli  # Computing the retention probability\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        # Performing forward propagation, applying dropout to the input data\n",
    "        self.input_data = input_data\n",
    "        # Creating a binary mask using Bernoulli distribution to drop neurons\n",
    "        self.binary_mask = np.random.binomial(1, self.p_bernoulli, size=input_data.shape) / self.p_bernoulli\n",
    "        # Applying dropout by element-wise multiplication with the binary mask\n",
    "        self.output = input_data * self.binary_mask\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_gradient, learning_rate):\n",
    "        # Performing backward propagation, applying dropout to the gradient\n",
    "        # Gradient passed through dropout is just the scaled gradient with the binary mask\n",
    "        return output_gradient * self.binary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dimension():\n",
    "    def __init__(self, input_dimension, output_dimension):\n",
    "        # Initializing the Dimension transformation with input and output dimensions\n",
    "        self.input_dimension = input_dimension  # Input dimension\n",
    "        self.output_dimension = output_dimension  # Output dimension\n",
    "    \n",
    "    def forward_propagation(self, input_data):\n",
    "        # Performing forward propagation by reshaping the input to the output dimension\n",
    "        return np.reshape(input_data, self.output_dimension)\n",
    "    \n",
    "    def backward_propagation(self, output_gradient, learning_rate):\n",
    "        # Performing backward propagation by reshaping the output gradient to the input dimension\n",
    "        return np.reshape(output_gradient, self.input_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2) Activation layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### * Sigmoide "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh():\n",
    "    def __init__(self):\n",
    "        # Defining the Tanh activation function and its derivative\n",
    "        tanh = lambda x: np.tanh(x)  # Tanh activation function\n",
    "        tanh_p = lambda x: 1 - np.tanh(x) ** 2  # Derivative of the Tanh function\n",
    "        self.activation = tanh  # Assigning the Tanh activation function\n",
    "        self.derivative_activation = tanh_p  # Assigning the derivative of the Tanh function\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        # Forward propagation: computing the output by applying the Tanh activation function\n",
    "        self.input = input_data  # Storing the input\n",
    "        return self.activation(self.input)  # Applying Tanh function to the input\n",
    "\n",
    "    def backward_propagation(self, output_gradient, learning_rate):\n",
    "        # Backward propagation: computing gradients using the derivative of the Tanh function\n",
    "        return np.multiply(output_gradient, self.derivative_activation(self.input))\n",
    "        # Element-wise multiplication of the output gradient by the derivative of Tanh to obtain the input gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### * Softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax():\n",
    "    def forward_propagation(self, input_data):\n",
    "        # Normalizing the input data using softmax function\n",
    "        maxi = np.max(input_data)  # Finding the maximum value in the input data\n",
    "        input_data = input_data - maxi  # Subtracting the maximum value for numerical stability\n",
    "        expo = np.exp(input_data)  # Calculating the exponentials of the normalized input\n",
    "        self.output = expo / np.sum(expo)  # Calculating the softmax probabilities\n",
    "        return self.output  # Returning the softmax output probabilities\n",
    "    \n",
    "    def backward_propagation(self, output_gradient, learning_rate):\n",
    "        # Backpropagation of softmax layer\n",
    "        # This layer often uses Cross-Entropy Loss (cce) as the error function\n",
    "        return output_gradient  # Transmitting the output gradient directly without further computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### * Relu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    def forward_propagation(self, input_data):\n",
    "        # Forward propagation: computes the output of the ReLU activation function\n",
    "        self.input = input_data  # Storing the input for later use\n",
    "        self.output = np.maximum(0, input_data)  # Applying ReLU activation\n",
    "        return self.output  # Returning the ReLU output\n",
    "    \n",
    "    def backward_propagation(self, output_gradient, learning_rate):\n",
    "        # Backward propagation: computes the gradient of the ReLU activation function\n",
    "        # Applying the derivative of ReLU to the output gradient\n",
    "        inter = output_gradient.copy()  # Creating a copy of the output gradient\n",
    "        inter[inter <= 0] = 0  # Replacing negative values in the gradient with 0\n",
    "        return inter  # Returning the computed gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3) Error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error (MSE)\n",
    "def mse(desired_output, output):\n",
    "    return np.mean(np.power(desired_output - output, 2))\n",
    "\n",
    "def mse_derivative(desired_output, output):\n",
    "    return 2 * (output - desired_output) / np.size(output)\n",
    "\n",
    "# Binary Cross Entropy (BCE)\n",
    "def bce(desired_output, output):\n",
    "    desired_output = np.clip(desired_output, 1e-7, 1 - 1e-7)  # Avoiding division by zero / log(0)\n",
    "    output = np.clip(output, 1e-7, 1 - 1e-7)  # Avoiding division by zero / log(0)\n",
    "    return -np.mean(desired_output * np.log(output) + (1 - desired_output) * np.log(1 - output))\n",
    "\n",
    "def bce_derivative(desired_output, output):\n",
    "    output = np.clip(output, 1e-7, 1 - 1e-7)\n",
    "    desired_output = np.clip(desired_output, 1e-7, 1 - 1e-7)\n",
    "    return ((1 - desired_output) / (1 - output) - desired_output / output) / np.size(output)\n",
    "\n",
    "# Categorical Cross Entropy (CCE)\n",
    "def cce(desired_output, output):\n",
    "    desired_output = np.clip(desired_output, 1e-7, 1 - 1e-7)\n",
    "    output = np.clip(output, 1e-7, 1 - 1e-7)\n",
    "    return -np.sum(np.log(output) * desired_output)\n",
    "\n",
    "def cce_derivative(desired_output, output):\n",
    "    desired_output = np.clip(desired_output, 1e-7, 1 - 1e-7)\n",
    "    output = np.clip(output, 1e-7, 1 - 1e-7)\n",
    "    return output - desired_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4) Neural Networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_error(res, input, output):\n",
    "    successes = 0  # Counter for successful predictions\n",
    "    total = 0  # Counter for total predictions\n",
    "    error_total = 0  # Total error\n",
    "\n",
    "    # Iterating through each entry in the dataset\n",
    "    for i in range(len(input)):\n",
    "        prediction = res.prediction(input[i])  # Obtaining the prediction using the model\n",
    "        error_total += res.error(output[i], prediction)  # Calculating the error for this prediction\n",
    "\n",
    "        # Checking if the index of the maximum value in the prediction matches the expected output\n",
    "        maxi = np.argmax(prediction)\n",
    "        if maxi == np.argmax(output[i]):\n",
    "            successes += 1  # Incrementing the success counter if the prediction is correct\n",
    "\n",
    "        total += 1  # Incrementing the total counter for predictions\n",
    "\n",
    "    # Calculating the accuracy (successes / total) and average error (error_total / total)\n",
    "    accuracy = successes / total\n",
    "    average_error = error_total / total\n",
    "\n",
    "    return (accuracy, average_error)  # Returning a tuple containing accuracy and average error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    def __init__(self, layers, error_function, error_derivative):\n",
    "        self.layers = layers  # Neural network layers\n",
    "        self.error = error_function  # Error function\n",
    "        self.error_derivative = error_derivative  # Derivative of the error function\n",
    "\n",
    "    def prediction(self, input_data):\n",
    "        output = input_data\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward_propagation(output)\n",
    "        return output\n",
    "\n",
    "    def training(self, input_train, output_train, input_test, output_test, iterations, learning_rate):\n",
    "        # Initialization of lists to store errors and precisions\n",
    "        error_list = []\n",
    "        error_list_test = []\n",
    "        precision_train = []\n",
    "        precision_test = []\n",
    "        \n",
    "        # Training loop\n",
    "        for iteration in range(iterations):\n",
    "            print(\"Iteration number:\", iteration + 1)\n",
    "            error = 0\n",
    "            start_time = time.time()\n",
    "            success_train = 0\n",
    "            total_train = 0\n",
    "            \n",
    "            # Training phase\n",
    "            for i in range(len(input_train)):\n",
    "                # Forward propagation\n",
    "                output = input_train[i]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "                \n",
    "                # Calculating accuracy\n",
    "                if np.argmax(output) == np.argmax(output_train[i]):\n",
    "                    success_train += 1\n",
    "                total_train += 1\n",
    "                \n",
    "                # Adding error\n",
    "                error += self.error(output_train[i], output)\n",
    "                \n",
    "                # Backpropagation\n",
    "                output_backward = self.error_derivative(output_train[i], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    output_backward = layer.backward_propagation(output_backward, learning_rate)\n",
    "            \n",
    "            # Error handling and calculation\n",
    "            error /= len(input_train)\n",
    "            error_list.append(error)\n",
    "            print(\"Error:\", error)\n",
    "            \n",
    "            # Computing error and accuracy on test set\n",
    "            test_accuracy, test_error = precision_error(self, input_test, output_test)\n",
    "            error_list_test.append(test_error)\n",
    "            print(\"Error on test set:\", error_list_test[-1])\n",
    "            \n",
    "            # Computing and storing training accuracy\n",
    "            precision_train.append(success_train / total_train)\n",
    "            precision_test.append(test_accuracy)\n",
    "            print(\"Accuracy on training set:\", precision_train[-1])\n",
    "            print(\"Accuracy on test set:\", precision_test[-1])\n",
    "            print(\"Iteration duration:\", round(time.time() - start_time, 2), \"s\")\n",
    "            print()\n",
    "\n",
    "        print(\"End of training\")\n",
    "        print(\"Training duration:\", round(time.time() - start_time, 2), \"s\")\n",
    "        \n",
    "        return (error_list, error_list_test, precision_train, precision_test)\n",
    "\n",
    "    def test(self, input_test, output_test):\n",
    "        error = 0\n",
    "        for i in range(len(input_test)):\n",
    "            # Forward propagation\n",
    "            output = input_test[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            \n",
    "            # Adding error\n",
    "            error += self.error(output_test[i], output)\n",
    "        \n",
    "        error /= len(input_test)\n",
    "        return error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) MNIST Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 17:21:31.550184: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist # MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number: 1\n",
      "Error: 2.3815929457956413\n",
      "Error on test set: 2.3223088988553133\n",
      "Accuracy on training set: 0.1026\n",
      "Accuracy on test set: 0.101001001001001\n",
      "Iteration duration: 1814.09 s\n",
      "\n",
      "Iteration number: 2\n",
      "Error: 2.3240918088586633\n",
      "Error on test set: 2.3223088988553133\n",
      "Accuracy on training set: 0.103\n",
      "Accuracy on test set: 0.101001001001001\n",
      "Iteration duration: 2020.53 s\n",
      "\n",
      "Iteration number: 3\n",
      "Error: 2.3240918088586633\n",
      "Error on test set: 2.3223088988553133\n",
      "Accuracy on training set: 0.103\n",
      "Accuracy on test set: 0.101001001001001\n",
      "Iteration duration: 1714.78 s\n",
      "\n",
      "Iteration number: 4\n",
      "Error: 2.3240918088586633\n",
      "Error on test set: 2.3223088988553133\n",
      "Accuracy on training set: 0.103\n",
      "Accuracy on test set: 0.101001001001001\n",
      "Iteration duration: 1589.64 s\n",
      "\n",
      "Iteration number: 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/lecourtoistristan/Documents/projet_IA/MNIST_classification/MNIST_Classification_From_Scratch.ipynb Cellule 24\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lecourtoistristan/Documents/projet_IA/MNIST_classification/MNIST_Classification_From_Scratch.ipynb#X32sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m network \u001b[39m=\u001b[39m Network(layers, cce, cce_derivative)  \u001b[39m# Initializing the neural network with layers and error functions\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lecourtoistristan/Documents/projet_IA/MNIST_classification/MNIST_Classification_From_Scratch.ipynb#X32sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# Training the neural network\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lecourtoistristan/Documents/projet_IA/MNIST_classification/MNIST_Classification_From_Scratch.ipynb#X32sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m train_error, test_error, train_accuracy, test_accuracy \u001b[39m=\u001b[39m network\u001b[39m.\u001b[39;49mtraining(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lecourtoistristan/Documents/projet_IA/MNIST_classification/MNIST_Classification_From_Scratch.ipynb#X32sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     input_train, output_train, input_test, output_test, \u001b[39m20\u001b[39;49m, \u001b[39m0.1\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lecourtoistristan/Documents/projet_IA/MNIST_classification/MNIST_Classification_From_Scratch.ipynb#X32sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lecourtoistristan/Documents/projet_IA/MNIST_classification/MNIST_Classification_From_Scratch.ipynb#X32sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# Plotting results\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lecourtoistristan/Documents/projet_IA/MNIST_classification/MNIST_Classification_From_Scratch.ipynb#X32sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure()\n",
      "\u001b[1;32m/Users/lecourtoistristan/Documents/projet_IA/MNIST_classification/MNIST_Classification_From_Scratch.ipynb Cellule 24\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lecourtoistristan/Documents/projet_IA/MNIST_classification/MNIST_Classification_From_Scratch.ipynb#X32sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     output_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror_derivative(output_train[i], output)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lecourtoistristan/Documents/projet_IA/MNIST_classification/MNIST_Classification_From_Scratch.ipynb#X32sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lecourtoistristan/Documents/projet_IA/MNIST_classification/MNIST_Classification_From_Scratch.ipynb#X32sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m         output_backward \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mbackward_propagation(output_backward, learning_rate)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lecourtoistristan/Documents/projet_IA/MNIST_classification/MNIST_Classification_From_Scratch.ipynb#X32sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# Error handling and calculation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lecourtoistristan/Documents/projet_IA/MNIST_classification/MNIST_Classification_From_Scratch.ipynb#X32sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m error \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(input_train)\n",
      "\u001b[1;32m/Users/lecourtoistristan/Documents/projet_IA/MNIST_classification/MNIST_Classification_From_Scratch.ipynb Cellule 24\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lecourtoistristan/Documents/projet_IA/MNIST_classification/MNIST_Classification_From_Scratch.ipynb#X32sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m             i_max, j_max \u001b[39m=\u001b[39m i_max[\u001b[39m0\u001b[39m], j_max[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lecourtoistristan/Documents/projet_IA/MNIST_classification/MNIST_Classification_From_Scratch.ipynb#X32sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m             \u001b[39m# Assigning the output gradients to the corresponding indices in the input gradients\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lecourtoistristan/Documents/projet_IA/MNIST_classification/MNIST_Classification_From_Scratch.ipynb#X32sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m             grad_input[c, i \u001b[39m*\u001b[39;49m stride: i \u001b[39m*\u001b[39;49m stride \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilter_h, j \u001b[39m*\u001b[39;49m stride: j \u001b[39m*\u001b[39;49m stride \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilter_w][i_max, j_max] \u001b[39m=\u001b[39m output_gradient[c, i, j]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lecourtoistristan/Documents/projet_IA/MNIST_classification/MNIST_Classification_From_Scratch.ipynb#X32sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mreturn\u001b[39;00m grad_input\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loading MNIST dataset\n",
    "(input_train, output_train), (input_test, output_test) = mnist.load_data()\n",
    "# input_test and output_test: Test input and output data\n",
    "\n",
    "# Data Processing\n",
    "def clean_mnist(input_data, output_data, limit):\n",
    "    # limit: Maximum number of examples per digit\n",
    "    indices = []\n",
    "    for i in range(10):\n",
    "        # Selecting indices up to the limit for each digit\n",
    "        indices.extend(np.where(output_data == i)[0][:limit])\n",
    "    indices = np.random.permutation(indices)\n",
    "    x = input_data[indices]\n",
    "    y = output_data[indices]\n",
    "    # Reshaping input data for network compatibility\n",
    "    x = x.reshape(len(x), 1, 28, 28)  # Adjusting image format to (depth, height, width)\n",
    "    x = x.astype(\"float32\") / 255  # Normalizing input data\n",
    "    identity_matrix = np.eye(10)  # Identity matrix of size 10\n",
    "    l = [identity_matrix[i] for i in y]\n",
    "    y = np.reshape(np.array(l), (len(y), 10, 1))\n",
    "    return x, y\n",
    "\n",
    "# Cleaning and preparing training and test data\n",
    "(input_train, output_train) = clean_mnist(input_train, output_train, 1000)\n",
    "(input_test, output_test) = clean_mnist(input_test, output_test, -1)\n",
    "\n",
    "# Neural Network Architecture\n",
    "# Layers: Dense (fully connected), Convolutional, Maxpooling, Dimension, Relu, Softmax\n",
    "layers = [\n",
    "    Convolution((1, 28, 28), 3, 32),  # Convolution layer with specified parameters\n",
    "    Relu(),  # Rectified Linear Unit activation\n",
    "    Maxpooling((32, 26, 26), (2, 2), 2),  # Maxpooling layer with specified parameters\n",
    "    Dimension((32, 13, 13), (32 * 13 * 13, 1)),  # Dimension layer mapping input to output size\n",
    "    Dense(32 * 13 * 13, 100),  # Fully connected layer with 100 neurons\n",
    "    Relu(),  # Activation function (ReLU)\n",
    "    Dense(100, 10),  # Output layer with 10 neurons for classification\n",
    "    Softmax()  # Softmax activation for probabilities\n",
    "]\n",
    "\n",
    "# Creating the neural network\n",
    "network = Network(layers, cce, cce_derivative)  # Initializing the neural network with layers and error functions\n",
    "\n",
    "# Training the neural network\n",
    "train_error, test_error, train_accuracy, test_accuracy = network.training(\n",
    "    input_train, output_train, input_test, output_test, 20, 0.1\n",
    ")\n",
    "\n",
    "# Plotting results\n",
    "plt.figure()\n",
    "plt.plot(range(1, 51), train_error)\n",
    "plt.plot(range(1, 51), test_error)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, 51), train_accuracy)\n",
    "plt.plot(range(1, 51), test_accuracy)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
